# Zen MCP Server Environment Configuration
# Copy this file to .env and fill in your values

# API Keys - At least one is required
#
# IMPORTANT: Choose ONE approach:
# - Native APIs (Gemini/OpenAI/XAI) for direct access
# - DIAL for unified enterprise access
# - OpenRouter for unified cloud access
# Having multiple unified providers creates ambiguity about which serves each model.
#
# Option 1: Use native APIs (recommended for direct access)
# Get your Gemini API key from: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# Get your OpenAI API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Use ChatGPT login tokens instead of OpenAI API key
# When enabled, the server will prioritize ChatGPT authentication tokens from ~/.codex/auth.json
# over the OPENAI_API_KEY above. This allows using your ChatGPT Pro subscription tokens.
# Requirements: 
#   - Valid ChatGPT Pro subscription with Codex access
#   - ~/.codex/auth.json file with valid tokens (access_token, account_id)
# Set to 'true' to enable, 'false' or leave empty to disable (default)
OPENAI_CHATGPT_LOGIN_MODE=false

# Get your X.AI API key from: https://console.x.ai/
XAI_API_KEY=your_xai_api_key_here

PERPLEXITY_API_KEY=your_perplexity_api_key_here
# Get your Perplexity API key from: https://www.perplexity.ai/settings/api
# Get your DIAL API key and configure host URL
# DIAL provides unified access to multiple AI models through a single API
DIAL_API_KEY=your_dial_api_key_here
# DIAL_API_HOST=https://core.dialx.ai        # Optional: Base URL without /openai suffix (auto-appended)
# DIAL_API_VERSION=2025-01-01-preview        # Optional: API version header for DIAL requests

# Option 2: Use OpenRouter for access to multiple models through one API
# Get your OpenRouter API key from: https://openrouter.ai/
# If using OpenRouter, comment out the native API keys above
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Option 3: Use custom API endpoints for local models (Ollama, vLLM, LM Studio, etc.)
# CUSTOM_API_URL=http://localhost:11434/v1  # Ollama example
# CUSTOM_API_KEY=                                      # Empty for Ollama (no auth needed)
# CUSTOM_MODEL_NAME=llama3.2                          # Default model name
# Optional custom timeouts (seconds) for custom endpoints
# CUSTOM_CONNECT_TIMEOUT=60
# CUSTOM_READ_TIMEOUT=1800
# CUSTOM_WRITE_TIMEOUT=1800
# CUSTOM_POOL_TIMEOUT=1800

# Vertex AI (optional, for Gemini via Vertex)
# If set, Vertex AI provider will be available
# VERTEX_PROJECT_ID=
# VERTEX_REGION=us-central1

# Optional: Default model to use
# Options: 'auto' (Claude picks best model), 'pro', 'flash', 'o3', 'o3-mini', 'o4-mini',
#          'gpt-5', 'gpt-5-mini', 'grok', 'opus-4', 'sonnet-4', or any DIAL model if DIAL is configured
# When set to 'auto', Claude will select the best model for each task
# Defaults to 'auto' if not specified
DEFAULT_MODEL=auto

# Optional: Default thinking mode for ThinkDeep tool
# NOTE: Only applies to models that support extended thinking (e.g., Gemini 2.5 Pro, GPT-5 models)
#       Flash models (2.0) will use system prompt engineering instead
# Token consumption per mode:
#   minimal: 128 tokens   - Quick analysis, fastest response
#   low:     2,048 tokens - Light reasoning tasks  
#   medium:  8,192 tokens - Balanced reasoning (good for most cases)
#   high:    16,384 tokens - Complex analysis (recommended for thinkdeep)
#   max:     32,768 tokens - Maximum reasoning depth, slowest but most thorough
# Defaults to 'high' if not specified
DEFAULT_THINKING_MODE_THINKDEEP=high

# Optional: Model usage restrictions
# Limit which models can be used from each provider for cost control, compliance, or standardization
# Format: Comma-separated list of allowed model names (case-insensitive, whitespace tolerant)
# Empty or unset = all models allowed (default behavior)
# If you want to disable a provider entirely, don't set its API key
#
# Supported OpenAI models:
#   - o3                  (200K context, high reasoning)
#   - o3-mini             (200K context, balanced)
#   - o3-pro              (200K context, professional-grade reasoning)
#   - o4-mini             (200K context, latest balanced, temperature fixed)
#   - gpt-4.1             (1M context, advanced reasoning)
#   - gpt-4.1-mini        (1M context, efficient variant)
#   - codex-mini-latest   (200K context, code-focused)
#   - gpt-5               (400K context, 128K output, reasoning tokens)
#   - gpt-5-mini          (400K context, 128K output, reasoning tokens)
#   - gpt-5-nano          (400K context, 128K output, reasoning tokens)
#   - Aliases: mini (gpt-5-mini), o3mini (o3-mini), o4mini (o4-mini), gpt5 (gpt-5), gpt5mini (gpt-5-mini)
#
# Supported Google/Gemini models:
#   - gemini-2.5-flash   (1M context, fast, supports thinking)
#   - gemini-2.5-pro     (1M context, powerful, supports thinking)
#   - flash              (shorthand for gemini-2.5-flash)
#   - pro                (shorthand for gemini-2.5-pro)
#
# Supported X.AI GROK models:
#   - grok-3          (131K context, advanced reasoning)
#   - grok-3-fast     (131K context, higher performance but more expensive)
#   - grok-4          (256K context, latest generation with multimodal capabilities)
#   - grok            (shorthand for grok-4)
#   - grok3           (shorthand for grok-3)
#   - grok4           (shorthand for grok-4)
#   - grokfast        (shorthand for grok-3-fast)
#
# Supported Perplexity models:
#   - sonar                 (127K context, fast search-augmented model)
#   - sonar-pro             (127K context, high-quality search-augmented model)
#   - sonar-reasoning       (127K context, reasoning with search capabilities)
#   - sonar-reasoning-pro   (127K context, advanced reasoning with search - enterprise)
#   - sonar-deep-research   (127K context, deep research for comprehensive analysis)
#   - r1-1776               (127K context, DeepSeek R1 post-trained, offline no search)
#   - perplexity                (shorthand for sonar)
#   - perplexity-pro            (shorthand for sonar-pro)
#   - perplexity-reasoning      (shorthand for sonar-reasoning)
#   - perplexity-reasoning-pro  (shorthand for sonar-reasoning-pro)
#   - deep-research             (shorthand for sonar-deep-research)
#   - r1                        (shorthand for r1-1776)
#
# Supported DIAL models (when available in your DIAL deployment):
#   - o3-2025-04-16   (200K context, latest O3 release)
#   - o4-mini-2025-04-16 (200K context, latest O4 mini)
#   - o3              (shorthand for o3-2025-04-16)
#   - o4-mini         (shorthand for o4-mini-2025-04-16)
#   - anthropic.claude-sonnet-4.1-20250805-v1:0 (200K context, Claude 4.1 Sonnet)
#   - anthropic.claude-sonnet-4.1-20250805-v1:0-with-thinking (200K context, Claude 4.1 Sonnet with thinking mode)
#   - anthropic.claude-opus-4.1-20250805-v1:0 (200K context, Claude 4.1 Opus)
#   - anthropic.claude-opus-4.1-20250805-v1:0-with-thinking (200K context, Claude 4.1 Opus with thinking mode)
#   - sonnet-4.1        (shorthand for Claude 4.1 Sonnet)
#   - sonnet-4.1-thinking (shorthand for Claude 4.1 Sonnet with thinking)
#   - opus-4.1          (shorthand for Claude 4.1 Opus)
#   - opus-4.1-thinking (shorthand for Claude 4.1 Opus with thinking)
#   - gemini-2.5-pro-preview-03-25-google-search (1M context, with Google Search)
#   - gemini-2.5-pro-preview-05-06 (1M context, latest preview)
#   - gemini-2.5-flash-preview-05-20 (1M context, latest flash preview)
#   - gemini-2.5-pro  (shorthand for gemini-2.5-pro-preview-05-06)
#   - gemini-2.5-pro-search (shorthand for gemini-2.5-pro-preview-03-25-google-search)
#   - gemini-2.5-flash (shorthand for gemini-2.5-flash-preview-05-20)
#
# Examples:
#   OPENAI_ALLOWED_MODELS=o3-mini,o4-mini,mini  # Only allow mini models (cost control)
#   GOOGLE_ALLOWED_MODELS=flash                  # Only allow Flash (fast responses)
#   XAI_ALLOWED_MODELS=grok-3                    # Only allow standard GROK (not fast variant)
#   OPENAI_ALLOWED_MODELS=o4-mini                # Single model standardization
#   GOOGLE_ALLOWED_MODELS=flash,pro              # Allow both Gemini models
#   XAI_ALLOWED_MODELS=grok,grok-3-fast          # Allow both GROK variants
#   DIAL_ALLOWED_MODELS=o3,o4-mini                       # Only allow O3/O4 models via DIAL
#   DIAL_ALLOWED_MODELS=opus-4.1,sonnet-4.1                  # Only Claude 4.1 models (without thinking)
#   DIAL_ALLOWED_MODELS=opus-4.1-thinking,sonnet-4.1-thinking # Only Claude 4.1 with thinking mode
#   DIAL_ALLOWED_MODELS=gemini-2.5-pro,gemini-2.5-flash  # Only Gemini 2.5 models via DIAL
#   PERPLEXITY_ALLOWED_MODELS=sonar,sonar-pro     # Only allow basic Sonar models
#   PERPLEXITY_ALLOWED_MODELS=sonar-reasoning,sonar-deep-research # Only reasoning models

#
# Note: These restrictions apply even in 'auto' mode - Claude will only pick from allowed models
# OPENAI_ALLOWED_MODELS=
# GOOGLE_ALLOWED_MODELS=
# XAI_ALLOWED_MODELS=
# PERPLEXITY_ALLOWED_MODELS=
# DIAL_ALLOWED_MODELS=

# Optional: Custom model configuration file path
# Override the default location of custom_models.json
# CUSTOM_MODELS_CONFIG_PATH=/path/to/your/custom_models.json

# Note: Conversations are stored in memory during the session

# Optional: Conversation timeout (hours)
# How long AI-to-AI conversation threads persist before expiring
# Longer timeouts use more memory but allow resuming conversations later
# Defaults to 3 hours if not specified
CONVERSATION_TIMEOUT_HOURS=3

# Optional: Max conversation turns
# Maximum number of turns allowed in an AI-to-AI conversation thread
# Each exchange (Claude asks, Gemini responds) counts as 2 turns
# So 20 turns = 10 exchanges. Defaults to 20 if not specified
MAX_CONVERSATION_TURNS=20

# Optional: Logging level (DEBUG, INFO, WARNING, ERROR)
# DEBUG: Shows detailed operational messages for troubleshooting (default)
# INFO: Shows general operational messages
# WARNING: Shows only warnings and errors
# ERROR: Shows only errors
LOG_LEVEL=DEBUG

# MCP Protocol Transport Limits (CLI ↔ MCP Server)
# Optional: Limit the maximum tokens returned to the CLI across the MCP boundary
# This protects CLI rendering buffers; server-internal processing is not limited by this.
# Example: 25000 (roughly ~100k characters)
# MAX_MCP_OUTPUT_TOKENS=

# Optional: Tool Selection
# Comma-separated list of tools to disable. If not set, all tools are enabled.
# Essential tools (version, listmodels) cannot be disabled.
# Available tools: chat, thinkdeep, planner, consensus, codereview, precommit,
#                  debug, docgen, analyze, refactor, tracer, testgen, challenge, secaudit
# 
# DEFAULT CONFIGURATION: To optimize context window usage, non-essential tools
# are disabled by default. Only the essential tools remain enabled:
# - chat, thinkdeep, planner, consensus (collaboration tools)
# - codereview, precommit, debug (code quality tools)  
# - challenge (critical thinking utility)
#
# To enable additional tools, remove them from the DISABLED_TOOLS list below.
DISABLED_TOOLS=analyze,refactor,testgen,secaudit,docgen,tracer

# Optional: Language/Locale for AI responses
# When set, all AI tools will respond in the specified language
# while maintaining their analytical capabilities
# Examples: "fr-FR", "en-US", "zh-CN", "zh-TW", "ja-JP", "ko-KR", "es-ES"
# Leave empty for default language (English)
# LOCALE=fr-FR

# ===========================================
# Docker Configuration
# ===========================================

# Container name for Docker Compose
# Used when running with docker-compose.yml
COMPOSE_PROJECT_NAME=zen-mcp

# Timezone for Docker containers
# Ensures consistent time handling in containerized environments
TZ=UTC

# Maximum log file size (default: 10MB)
# Applicable when using file-based logging
LOG_MAX_SIZE=10MB

# ===========================================
# Research Tool Configuration
# ===========================================
# Default model to use (sonar, sonar-pro, sonar-reasoning, sonar-reasoning-pro, sonar-deep-research)
RESEARCH_DEFAULT_MODEL=sonar-pro

# Default search mode (web, high, medium, low)
RESEARCH_DEFAULT_SEARCH_MODE=medium

# Default maximum tokens for research responses
RESEARCH_DEFAULT_MAX_TOKENS=2048

# Export research results to markdown files
RESEARCH_EXPORT_TO_MD=false

# Directory for research exports (relative to project root)
RESEARCH_EXPORT_DIR=research_exports
